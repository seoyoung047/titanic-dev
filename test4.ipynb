{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 함수 모음\n",
    "\n",
    "## train, test 불러오기\n",
    "def load_traintest_dt():\n",
    "    train = pd.read_csv('../data/titanic/train.csv')\n",
    "    test = pd.read_csv('../data/titanic/test.csv')  \n",
    "\n",
    "    return (train, test)\n",
    "\n",
    "## submission_a 불러오기\n",
    "def load_submission_a() :\n",
    "    submission_a = pd.read_csv('../data/submission_a.csv')\n",
    "    del submission_a['PassengerId']\n",
    "\n",
    "    return submission_a\n",
    "\n",
    "## 피처 삭제\n",
    "def drop_col(train,test,col_list):\n",
    "    '''\n",
    "    drop train, test column\n",
    "    parameter : train,test, list\n",
    "    return train,test\n",
    "    '''\n",
    "    train = train.drop(col_list,axis=1)\n",
    "    test = test.drop(col_list, axis=1)\n",
    "    return (train,test)\n",
    "\n",
    "## train, target 나누기\n",
    "def split_data(train):\n",
    "    train_data = train.drop('Survived',axis=1)\n",
    "    target = train['Survived']\n",
    "\n",
    "    return (train_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_a = load_submission_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_traintest_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "\n",
    "def Name_cleaning(train,test):\n",
    "    \n",
    "    # Name,Title\n",
    "    train_test_data = [train,test]\n",
    "    for dataset in train_test_data:\n",
    "        dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.',expand=False)\n",
    "\n",
    "    train_title_mapping = {'Mr':0, 'Miss':1, 'Mrs':2, 'Master':3\n",
    "                    , 'Dr':4, 'Rev':4, 'Mlle':4, 'Major':4, 'Col':4\n",
    "                    ,'Countess':4, 'Capt':4, 'Ms':4, 'Sir':4, 'Lady':4\n",
    "                    , 'Mme':4, 'Don':4, 'Jonkheer':4\n",
    "                    }\n",
    "    train['Title'] = train['Title'].map(train_title_mapping)\n",
    "\n",
    "    test_title_mapping = {'Mr':0, 'Miss':1, 'Mrs':2, 'Master':3\n",
    "                    , 'Col':4, 'Rev':4, 'Ms':4, 'Dr':4, 'Dona':4\n",
    "                    }\n",
    "\n",
    "    test['Title'] = test['Title'].map(test_title_mapping)\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def sex_cleaning(train,test):\n",
    "    # sex\n",
    "    sex_mapping = {'male':0, 'female':1}\n",
    "    train_test_data = [train,test]\n",
    "    for dataset in train_test_data:\n",
    "        dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def haveage_cleaning(train,test):\n",
    "    # Have Age\n",
    "\n",
    "    train.loc[train['Age'].isnull(), 'Null_Age'] = 0\n",
    "    test.loc[test['Age'].isnull(), 'Null_Age'] = 0\n",
    "\n",
    "    train.loc[train['Age'].notnull(), 'Null_Age'] = 1\n",
    "    test.loc[test['Age'].notnull(), 'Null_Age'] = 1\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "\n",
    "def age_cleaning(train,test):\n",
    "    # Age\n",
    "    \n",
    "    train['Age'].fillna(train.groupby('Title')['Age'].transform('mean'),inplace=True)\n",
    "    test['Age'].fillna(test.groupby('Title')['Age'].transform('mean'),inplace=True)\n",
    "\n",
    "    train_test_data = [train,test]\n",
    "    for dataset in train_test_data:\n",
    "        dataset.loc[dataset['Age'] <= 17, 'Age'] =0\n",
    "        dataset.loc[(dataset['Age'] >17) & (dataset['Age'] <= 24), 'Age'] = 1\n",
    "        dataset.loc[(dataset['Age'] >24) & (dataset['Age'] <= 34), 'Age'] = 2\n",
    "        dataset.loc[(dataset['Age'] >34) & (dataset['Age'] <= 44), 'Age'] = 3\n",
    "        dataset.loc[(dataset['Age'] >44) & (dataset['Age'] <= 60), 'Age'] = 4\n",
    "        dataset.loc[dataset['Age'] >60, 'Age'] = 5\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "\n",
    "def embarked_cleaning(train,test):\n",
    "    # Embarked\n",
    "\n",
    "    train['Embarked'] = train['Embarked'].fillna('S')\n",
    "    test['Embarked'] = test['Embarked'].fillna('S')\n",
    "\n",
    "    embarked_mapping = {'S':0, 'C':1, 'Q':2}\n",
    "    train['Embarked'] = train['Embarked'].map(embarked_mapping)\n",
    "    test['Embarked'] = test['Embarked'].map(embarked_mapping)\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def fare_cleaning(train,test):\n",
    "    #Fare\n",
    "\n",
    "    test['Fare'].fillna(\n",
    "        test.groupby('Pclass')['Fare'].transform('median'), inplace=True\n",
    "    )\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def groupsize_cleanig(train,test):\n",
    "    # group_size\n",
    "\n",
    "    for ticket_num in train['Ticket'].unique():\n",
    "        train.loc[train['Ticket']==ticket_num,'group_size'] = len(train[train['Ticket']==ticket_num])\n",
    "\n",
    "    for ticket_num in test['Ticket'].unique():\n",
    "        test.loc[test['Ticket']==ticket_num,'group_size'] = len(test[test['Ticket']==ticket_num])\n",
    "\n",
    "    train['Fare'] = train['Fare']/train['group_size']\n",
    "    test['Fare'] = test['Fare']/test['group_size']\n",
    "    \n",
    "    train_test_data = [train,test]\n",
    "    for dataset in train_test_data:\n",
    "        dataset.loc[dataset['group_size'] == 1, 'group_size'] = 0\n",
    "        dataset.loc[dataset['group_size'] == 2, 'group_size'] = 0.4\n",
    "        dataset.loc[(dataset['group_size'] == 3) | (dataset['group_size'] == 4), 'group_size'] = 0.8\n",
    "        dataset.loc[dataset['group_size'] > 4, 'group_size'] = 1.2\n",
    "\n",
    "    train_test_data = [train,test]\n",
    "    for dataset in train_test_data:\n",
    "\n",
    "        dataset.loc[dataset['Fare'] <= 7,'Fare'] = 0\n",
    "        dataset.loc[(dataset['Fare'] >7) & (dataset['Fare'] <=8.8), 'Fare'] = 0.4\n",
    "        dataset.loc[(dataset['Fare'] >8.8) & (dataset['Fare'] <=17), 'Fare'] = 0.8\n",
    "        dataset.loc[(dataset['Fare'] >17) & (dataset['Fare'] <=30), 'Fare'] = 1.2\n",
    "        dataset.loc[(dataset['Fare'] >30) & (dataset['Fare'] <=100), 'Fare'] = 1.6\n",
    "        dataset.loc[dataset['Fare'] > 100,'Fare'] = 2\n",
    "        \n",
    "    return (train,test)\n",
    "\n",
    "def havecabin_cleaning(train,test):\n",
    "    # Have Cabin\n",
    "\n",
    "    train.loc[train['Cabin'].isnull(), 'Null_Cabin'] = 0\n",
    "    test.loc[test['Cabin'].isnull(), 'Null_Cabin'] = 0\n",
    "\n",
    "    train.loc[train['Cabin'].notnull(), 'Null_Cabin'] = 1\n",
    "    test.loc[test['Cabin'].notnull(), 'Null_Cabin'] = 1\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def cabin_cleaning(train,test):\n",
    "    # Cabin\n",
    "\n",
    "    train['Cabin'] = train['Cabin'].str[:1]\n",
    "    test['Cabin'] = test['Cabin'].str[:1]\n",
    "\n",
    "    cabin_mapping = {\"A\": 0, \"B\": 0.4\n",
    "                    , \"C\": 0.8, \"D\": 1.2\n",
    "                    , \"E\": 1.6, \"F\": 2, \"G\": 2.4\n",
    "                    , 'T' :2.8\n",
    "                    }\n",
    "\n",
    "    train['Cabin'] = train['Cabin'].map(cabin_mapping)\n",
    "    test['Cabin'] = test['Cabin'].map(cabin_mapping)\n",
    "\n",
    "    train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\n",
    "    test[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def familysize_cleaning(train,test):\n",
    "    # FamilySize\n",
    "\n",
    "    train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "    test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
    "\n",
    "    family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\n",
    "    train['FamilySize'] = train['FamilySize'].map(family_mapping)\n",
    "    test['FamilySize'] = test['FamilySize'].map(family_mapping)\n",
    "\n",
    "    return (train,test)\n",
    "\n",
    "def data_scaler(train,test):\n",
    "    # 정규화\n",
    "\n",
    "    columns = ['Pclass', 'Sex', 'Age', 'Fare', 'Cabin',\n",
    "        'Embarked', 'Title', 'Null_Age', 'group_size', 'Null_Cabin',\n",
    "        'FamilySize']\n",
    " \n",
    "    train_test_data = [train,test]\n",
    "    for dataset in train_test_data:\n",
    "        scaler = MinMaxScaler()\n",
    "        # scaler = StandardScaler()\n",
    "\n",
    "        scaler.fit(dataset[columns])\n",
    "        scaled = scaler.transform(dataset[columns])\n",
    "\n",
    "        df_scaled = pd.DataFrame(data=scaled, columns=columns)\n",
    "        dataset.loc[:,columns] = df_scaled\n",
    "\n",
    "    return (train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 전체 실행\n",
    "\n",
    "def data_cleaning():\n",
    "    train,test = load_traintest_dt()\n",
    "\n",
    "    #name\n",
    "    train,test = Name_cleaning(train,test)\n",
    "    #sex\n",
    "    train,test = sex_cleaning(train,test)\n",
    "    #haveage\n",
    "    train,test = haveage_cleaning(train,test)\n",
    "    #age\n",
    "    train,test = age_cleaning(train,test)\n",
    "    #embarked\n",
    "    train,test = embarked_cleaning(train,test)\n",
    "    #fare\n",
    "    train,test = fare_cleaning(train,test)\n",
    "    #groupsize\n",
    "    train,test = groupsize_cleanig(train,test)\n",
    "    #havecabin\n",
    "    train,test = havecabin_cleaning(train,test)\n",
    "    #cabin\n",
    "    train,test = cabin_cleaning(train,test)\n",
    "    #familysize\n",
    "    train,test = familysize_cleaning(train,test)\n",
    "    \n",
    "    #scaler\n",
    "    train,test = data_scaler(train,test)\n",
    "\n",
    "    #drop\n",
    "    train,test = drop_col(train,test,['Ticket','SibSp','Parch','Name'])\n",
    "\n",
    "    #split\n",
    "\n",
    "    train_data, target = split_data(train)\n",
    "\n",
    "    return (train_data,target,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, target, test = data_cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Null_Age</th>\n",
       "      <th>group_size</th>\n",
       "      <th>Null_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>0.632775</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.432057</td>\n",
       "      <td>0.702951</td>\n",
       "      <td>0.232057</td>\n",
       "      <td>0.187201</td>\n",
       "      <td>0.794258</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.217703</td>\n",
       "      <td>0.083971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>120.810458</td>\n",
       "      <td>0.420919</td>\n",
       "      <td>0.481622</td>\n",
       "      <td>0.248545</td>\n",
       "      <td>0.248209</td>\n",
       "      <td>0.234738</td>\n",
       "      <td>0.342758</td>\n",
       "      <td>0.254641</td>\n",
       "      <td>0.404727</td>\n",
       "      <td>0.214503</td>\n",
       "      <td>0.413179</td>\n",
       "      <td>0.151907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>892.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>996.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1204.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Sex         Age        Fare   \n",
       "count   418.000000  418.000000  418.000000  418.000000  418.000000  \\\n",
       "mean   1100.500000    0.632775    0.363636    0.400000    0.432057   \n",
       "std     120.810458    0.420919    0.481622    0.248545    0.248209   \n",
       "min     892.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%     996.250000    0.000000    0.000000    0.200000    0.200000   \n",
       "50%    1100.500000    1.000000    0.000000    0.400000    0.400000   \n",
       "75%    1204.750000    1.000000    1.000000    0.600000    0.600000   \n",
       "max    1309.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "            Cabin    Embarked       Title    Null_Age  group_size  Null_Cabin   \n",
       "count  418.000000  418.000000  418.000000  418.000000  418.000000  418.000000  \\\n",
       "mean     0.702951    0.232057    0.187201    0.794258    0.106061    0.217703   \n",
       "std      0.234738    0.342758    0.254641    0.404727    0.214503    0.413179   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.666667    0.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "50%      0.833333    0.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "75%      0.833333    0.500000    0.250000    1.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       FamilySize  \n",
       "count  418.000000  \n",
       "mean     0.083971  \n",
       "std      0.151907  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      0.100000  \n",
       "max      1.000000  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Null_Age</th>\n",
       "      <th>group_size</th>\n",
       "      <th>Null_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass  Sex  Age  Fare     Cabin  Embarked  Title  Null_Age   \n",
       "0            1     1.0    0  0.2   0.2  0.714286       0.0   0.00       1.0  \\\n",
       "1            2     0.0    1  0.6   0.8  0.285714       0.5   0.50       1.0   \n",
       "2            3     1.0    1  0.4   0.2  0.714286       0.0   0.25       1.0   \n",
       "3            4     0.0    1  0.6   0.6  0.285714       0.0   0.50       1.0   \n",
       "4            5     1.0    0  0.6   0.2  0.714286       0.0   0.00       1.0   \n",
       "\n",
       "   group_size  Null_Cabin  FamilySize  \n",
       "0    0.000000         0.0         0.1  \n",
       "1    0.000000         1.0         0.1  \n",
       "2    0.000000         0.0         0.0  \n",
       "3    0.333333         1.0         0.1  \n",
       "4    0.000000         0.0         0.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name | Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train,test]\n",
    "\n",
    "for dataset in train_test_data:\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.',expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace the titles with only the simple ones.\n",
    "\n",
    "for df in train_test_data :\n",
    "    male_dr_filter = (df.Title == 'Dr') & (df.Sex == 'male')\n",
    "    female_dr_filter = (df.Title == 'Dr') & (df.Sex == 'female')\n",
    "    df.loc[male_dr_filter, ['Title']] = 'Mr'\n",
    "    df.loc[female_dr_filter, ['Title']] = 'Mrs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_title_mapping = {'Mr':0, 'Miss':1, 'Mrs':2, 'Master':3\n",
    "                , 'Dr':4, 'Rev':4, 'Mlle':4, 'Major':4, 'Col':4\n",
    "                ,'Countess':4, 'Capt':4, 'Ms':4, 'Sir':4, 'Lady':4\n",
    "                , 'Mme':4, 'Don':4, 'Jonkheer':4\n",
    "                }\n",
    "train['Title'] = train['Title'].map(train_title_mapping)\n",
    "\n",
    "test_title_mapping = {'Mr':0, 'Miss':1, 'Mrs':2, 'Master':3\n",
    "                , 'Col':4, 'Rev':4, 'Ms':4, 'Dr':4, 'Dona':4\n",
    "                }\n",
    "\n",
    "test['Title'] = test['Title'].map(test_title_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_mapping = {'male':0, 'female':1}\n",
    "\n",
    "for dataset in train_test_data:\n",
    "    dataset['Sex'] = dataset['Sex'].map(sex_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Age'].isnull(), 'Null_Age'] = 0\n",
    "test.loc[test['Age'].isnull(), 'Null_Age'] = 0\n",
    "\n",
    "train.loc[train['Age'].notnull(), 'Null_Age'] = 1\n",
    "test.loc[test['Age'].notnull(), 'Null_Age'] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing age with median age for each title(Mr,Mrs,Miss,Master,Others)\n",
    "# Age 결측치 해당 Title의 나이의 중앙값으로 채우기\n",
    "\n",
    "train['Age'].fillna(train.groupby('Title')['Age'].transform('mean'),inplace=True)\n",
    "test['Age'].fillna(test.groupby('Title')['Age'].transform('mean'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in train_test_data:\n",
    "    dataset.loc[dataset['Age'] <= 17, 'Age'] =0\n",
    "    dataset.loc[(dataset['Age'] >17) & (dataset['Age'] <= 24), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] >24) & (dataset['Age'] <= 34), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] >34) & (dataset['Age'] <= 44), 'Age'] = 3\n",
    "    dataset.loc[(dataset['Age'] >44) & (dataset['Age'] <= 60), 'Age'] = 4\n",
    "    dataset.loc[dataset['Age'] >60, 'Age'] = 5\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대부분 S embark 에서 탐 => fillna('S')\n",
    "\n",
    "train['Embarked'] = train['Embarked'].fillna('S')\n",
    "test['Embarked'] = test['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_mapping = {'S':0, 'C':1, 'Q':2}\n",
    "train['Embarked'] = train['Embarked'].map(embarked_mapping)\n",
    "test['Embarked'] = test['Embarked'].map(embarked_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Fare'].fillna(\n",
    "    test.groupby('Pclass')['Fare'].transform('median'), inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'].isnull().sum(), test['Fare'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticket_num in train['Ticket'].unique():\n",
    "    train.loc[train['Ticket']==ticket_num,'group_size'] = len(train[train['Ticket']==ticket_num])\n",
    "\n",
    "for ticket_num in test['Ticket'].unique():\n",
    "    test.loc[test['Ticket']==ticket_num,'group_size'] = len(test[test['Ticket']==ticket_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'] = train['Fare']/train['group_size']\n",
    "test['Fare'] = test['Fare']/test['group_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train,test]\n",
    "\n",
    "for dataset in train_test_data:\n",
    "    dataset.loc[dataset['group_size'] == 1, 'group_size'] = 0\n",
    "    dataset.loc[dataset['group_size'] == 2, 'group_size'] = 0.4\n",
    "    dataset.loc[(dataset['group_size'] == 3) | (dataset['group_size'] == 4), 'group_size'] = 0.8\n",
    "    dataset.loc[dataset['group_size'] > 4, 'group_size'] = 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# # 사이킷런의 StandardScaler를 이용하여 정규분포 형태로 피처값 변환하는 로직으로 수정. \n",
    "# def get_preprocessed_df(df=None):\n",
    "#     df_copy = df.copy()\n",
    "#     scaler = StandardScaler()\n",
    "#     amount_n = scaler.fit_transform(df_copy['Fare'].values.reshape(-1, 1))\n",
    "#     # 피처명 변경후 DataFrame맨 앞 컬럼으로 입력\n",
    "#     df_copy.insert(0, 'Fare_Scaled', amount_n)\n",
    "#     # 기존 피처 삭제\n",
    "#     df_copy.drop(['Fare'], axis=1, inplace=True)\n",
    "#     return df_copy\n",
    "\n",
    "# train = get_preprocessed_df(train)\n",
    "# test =  get_preprocessed_df(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Fare_Scaled'].max(),train['Fare_Scaled'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Fare_Scaled'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# # plt.xticks(range(0, 3000, 1000), rotation=60)\n",
    "# sns.histplot(train['Fare_Scaled'], kde=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_data = [train,test]\n",
    "# for dataset in train_test_data:\n",
    "\n",
    "#     dataset.loc[dataset['Fare_Scaled'] <= -4.5,'Fare_Scaled'] = 0\n",
    "#     dataset.loc[(dataset['Fare_Scaled'] >-4.5) & (dataset['Fare_Scaled'] <=-4.2), 'Fare_Scaled'] = 0.4\n",
    "#     dataset.loc[(dataset['Fare_Scaled'] >-4.2) & (dataset['Fare_Scaled'] <=0), 'Fare_Scaled'] = 0.8\n",
    "#     dataset.loc[(dataset['Fare_Scaled'] >0) & (dataset['Fare_Scaled'] <=1), 'Fare_Scaled'] = 1.2\n",
    "#     dataset.loc[(dataset['Fare_Scaled'] >1) & (dataset['Fare_Scaled'] <=4), 'Fare_Scaled'] = 1.6\n",
    "#     dataset.loc[dataset['Fare_Scaled'] > 4,'Fare_Scaled'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# # plt.xticks(range(0, 30000, 1000), rotation=60)\n",
    "# sns.histplot(train['Fare'], kde=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [train,test]\n",
    "for dataset in train_test_data:\n",
    "\n",
    "    dataset.loc[dataset['Fare'] <= 7,'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] >7) & (dataset['Fare'] <=8.8), 'Fare'] = 0.4\n",
    "    dataset.loc[(dataset['Fare'] >8.8) & (dataset['Fare'] <=17), 'Fare'] = 0.8\n",
    "    dataset.loc[(dataset['Fare'] >17) & (dataset['Fare'] <=30), 'Fare'] = 1.2\n",
    "    # dataset.loc[dataset['Fare']>30, 'Fare'] = 1.6\n",
    "    dataset.loc[(dataset['Fare'] >30) & (dataset['Fare'] <=100), 'Fare'] = 1.6\n",
    "    dataset.loc[dataset['Fare'] > 100,'Fare'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Cabin'].isnull(), 'Null_Cabin'] = 0\n",
    "test.loc[test['Cabin'].isnull(), 'Null_Cabin'] = 0\n",
    "\n",
    "train.loc[train['Cabin'].notnull(), 'Null_Cabin'] = 1\n",
    "test.loc[test['Cabin'].notnull(), 'Null_Cabin'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Null_Cabin'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabin_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Cabin'].str.extract('([0-9]+)',expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Cabin_num'] = train['Cabin'].str.extract('([0-9]+)',expand=False)\n",
    "# test['Cabin_num'] = test['Cabin'].str.extract('([0-9]+)',expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['Cabin_num'] = train[train['Cabin_num'].notnull()]['Cabin_num'].astype(int)\n",
    "# test['Cabin_num'] = test[test['Cabin_num'].notnull()]['Cabin_num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"Cabin_num\"].fillna(train.groupby(\"Pclass\")[\"Cabin_num\"].transform(\"median\"), inplace=True)\n",
    "# test[\"Cabin_num\"].fillna(test.groupby(\"Pclass\")[\"Cabin_num\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train['Cabin_num'] <= 22,'Cabin_num'] = 0\n",
    "# train.loc[(train['Cabin_num'] >22) & (train['Cabin_num'] <=43), 'Cabin_num'] = 1\n",
    "# train.loc[(train['Cabin_num'] >43) & (train['Cabin_num'] <=70), 'Cabin_num'] = 2\n",
    "# train.loc[train['Cabin_num'] > 70,'Cabin_num'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.loc[test['Cabin_num'] <= 22,'Cabin_num'] = 0\n",
    "# test.loc[(test['Cabin_num'] >22) & (test['Cabin_num'] <=43), 'Cabin_num'] = 1\n",
    "# test.loc[(test['Cabin_num'] >43) & (test['Cabin_num'] <=70), 'Cabin_num'] = 2\n",
    "# test.loc[test['Cabin_num'] > 70,'Cabin_num'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin'] = train['Cabin'].str[:1]\n",
    "test['Cabin'] = test['Cabin'].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop(index=train[train['Cabin']=='T'].index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_mapping = {\"A\": 0, \"B\": 0.4\n",
    "                 , \"C\": 0.8, \"D\": 1.2\n",
    "                 , \"E\": 1.6, \"F\": 2, \"G\": 2.4\n",
    "                 , 'T' :2.8\n",
    "                 }\n",
    "\n",
    "train['Cabin'] = train['Cabin'].map(cabin_mapping)\n",
    "test['Cabin'] = test['Cabin'].map(cabin_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\n",
    "test[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FamilySize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\n",
    "train['FamilySize'] = train['FamilySize'].map(family_mapping)\n",
    "test['FamilySize'] = test['FamilySize'].map(family_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero_Fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[train['FamilySize']==0, 'Zero_Fam'] = 0\n",
    "# test.loc[train['FamilySize']==0, 'Zero_Fam'] = 0\n",
    "\n",
    "# train.loc[train['FamilySize']>0, 'Zero_Fam'] = 1\n",
    "# test.loc[train['FamilySize']>0, 'Zero_Fam'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.get_dummies(test_df,columns=['Title', 'FamilySize'], drop_first=True)\n",
    "# train_df = pd.get_dummies(train_df,columns=['Title', 'FamilySize'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Pclass', 'Sex', 'Age', 'Fare', 'Cabin',\n",
    "       'Embarked', 'Title', 'Null_Age', 'group_size', 'Null_Cabin',\n",
    "       'FamilySize']\n",
    "# 객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "# 데이터 셑 변환, fit(), transform()\n",
    "scaler.fit(train[columns])\n",
    "scaled = scaler.transform(train[columns])\n",
    "\n",
    "#transforma()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "df_scaled = pd.DataFrame(data=scaled, columns=columns)\n",
    "\n",
    "print('최솟값')             # 0에 가까워짐\n",
    "print(df_scaled.min())\n",
    "print('\\n최댓값')\n",
    "print(df_scaled.max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,columns] = df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Pclass', 'Sex', 'Age', 'Fare', 'Cabin',\n",
    "       'Embarked', 'Title', 'Null_Age', 'group_size', 'Null_Cabin',\n",
    "       'FamilySize']\n",
    "# 객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "# 데이터 셑 변환, fit(), transform()\n",
    "scaler.fit(test[columns])\n",
    "scaled = scaler.transform(test[columns])\n",
    "\n",
    "#transforma()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "df_scaled = pd.DataFrame(data=scaled, columns=columns)\n",
    "\n",
    "print('최솟값')             # 0에 가까워짐\n",
    "print(df_scaled.min())\n",
    "print('\\n최댓값')\n",
    "print(df_scaled.max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:,columns] = df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Fare'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "corr = train.corr()\n",
    "sns.heatmap(corr, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RandomForest\n",
    "# parameters = {'n_estimators':[400,500]\n",
    "#               ,'max_depth':[4,6,8]\n",
    "#               ,'min_samples_split':[2,4,6]\n",
    "#               ,'min_samples_leaf' : [2,4,6]\n",
    "#               , 'random_state':[32,49]\n",
    "#               }\n",
    "\n",
    "# rf_clf = RandomForestClassifier()\n",
    "\n",
    "# grid_clf = GridSearchCV(rf_clf,param_grid=parameters,scoring='accuracy',cv=5,n_jobs=-1)\n",
    "\n",
    "# grid_clf.fit(train_data,target)\n",
    "# print(grid_clf.best_params_)\n",
    "# print(grid_clf.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=650\n",
    "                             , max_depth=5\n",
    "                            #  ,min_samples_split=2\n",
    "                             ,min_samples_leaf=14\n",
    "                             , random_state=1)\n",
    "clf.fit(train_data, target)\n",
    "\n",
    "prediction = clf.predict(test)\n",
    "\n",
    "score = cross_val_score(clf, train_data, target, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "print(score.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from xgboost import plot_importance\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 불균형 데이터셋\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "# from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "#     confusion = confusion_matrix( y_test, pred)\n",
    "#     accuracy = accuracy_score(y_test , pred)\n",
    "#     precision = precision_score(y_test , pred)\n",
    "#     recall = recall_score(y_test , pred)\n",
    "#     f1 = f1_score(y_test,pred)\n",
    "#     # ROC-AUC 추가 \n",
    "#     roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "#     print('오차 행렬')\n",
    "#     print(confusion)\n",
    "#     # ROC-AUC print 추가\n",
    "#     print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "#     F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# parameters = {'n_estimators':[900,1000]\n",
    "#             , 'learning_rate':[0.001,0.002,0.003]\n",
    "#             , 'max_depth':[4,5]\n",
    "#             , 'reg_lambda':[3,4,5]\n",
    "#             , 'random_state':[1]\n",
    "#                             }\n",
    "\n",
    "# xgb_wrapper = XGBClassifier()\n",
    "\n",
    "# grid_clf = GridSearchCV(xgb_wrapper,param_grid=parameters,scoring='accuracy',cv=5,n_jobs=-1)\n",
    "\n",
    "\n",
    "# grid_clf.fit(train_data, target\n",
    "#                 # , early_stopping_rounds=50\n",
    "#                 # , eval_metric=\"logloss\"\n",
    "#                 # , eval_set=evals\n",
    "#                 , verbose=False  # 결과 추출\n",
    "#                 )\n",
    "\n",
    "# print(grid_clf.best_params_)\n",
    "# print(grid_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = XGBClassifier(learning_rate=0.002\n",
    "#                              , max_depth=4\n",
    "#                              , n_estimators=900\n",
    "#                              , reg_lambda=3\n",
    "#                              , random_state=1\n",
    "#                              , n_jobs=-1)\n",
    "# clf.fit(train_data, target\n",
    "#         , verbose=False  # 결과 추출\n",
    "#         )\n",
    "\n",
    "# prediction = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_list=[]\n",
    "\n",
    "# for r in range(1,100):\n",
    "#     clf = XGBClassifier(learning_rate=(r/100000)\n",
    "#                         , max_depth=4\n",
    "#                         , n_estimators=900\n",
    "#                         , reg_lambda=15\n",
    "                      \n",
    "#                         )\n",
    "#     clf.fit(train_data, target\n",
    "#             , verbose=False  # 결과 추출\n",
    "#             )\n",
    "#     Y_pred = clf.predict(test) # 테스트 데이터로 예측값 추출    \n",
    "    \n",
    "#     accuracy = accuracy_score(Y_pred, submission_a)\n",
    "#     print(r, accuracy)\n",
    "#     acc_list.append(accuracy)\n",
    "\n",
    "# acc_list.index(max(acc_list)),max(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = accuracy_score(prediction, submission_a)\n",
    "\n",
    "# accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.7511961722488039\n",
      "3 0.7607655502392344\n",
      "4 0.7631578947368421\n",
      "5 0.7607655502392344\n",
      "6 0.7607655502392344\n",
      "7 0.7607655502392344\n",
      "8 0.7631578947368421\n",
      "9 0.7631578947368421\n",
      "10 0.7607655502392344\n",
      "11 0.7607655502392344\n",
      "12 0.7607655502392344\n",
      "13 0.7607655502392344\n",
      "14 0.7607655502392344\n",
      "15 0.7607655502392344\n",
      "16 0.7607655502392344\n",
      "17 0.7607655502392344\n",
      "18 0.7607655502392344\n",
      "19 0.7655502392344498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17, 0.7655502392344498)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_list=[]\n",
    "\n",
    "for r in range(2,20):\n",
    "    clf = RandomForestClassifier(n_estimators=280   \n",
    "                                 , max_depth=4\n",
    "                                #  , min_samples_split=r\n",
    "                                 , min_samples_leaf=r\n",
    "                                 , random_state=1\n",
    "                                 , n_jobs=-1)\n",
    "    clf.fit(train_data, target) # 학습\n",
    "    Y_pred = clf.predict(test) # 테스트 데이터로 예측값 추출    \n",
    "    \n",
    "    accuracy = accuracy_score(Y_pred, submission_a)\n",
    "    print(r, accuracy)\n",
    "    acc_list.append(accuracy)\n",
    "\n",
    "acc_list.index(max(acc_list)),max(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=122\n",
    "                             , max_depth=5\n",
    "                             #,min_samples_split=6\n",
    "                             ,min_samples_leaf=18\n",
    "                             , random_state=793\n",
    "                             ,n_jobs=-1)\n",
    "clf.fit(train_data, target)\n",
    "\n",
    "prediction = clf.predict(test)\n",
    "\n",
    "accuracy = accuracy_score(prediction, submission_a)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, target, test = data_cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8157894736842105"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=660\n",
    "                             , max_depth=5\n",
    "                             #,min_samples_split=6\n",
    "                             ,min_samples_leaf=14\n",
    "                             , random_state=1\n",
    "                             ,n_jobs=-1)\n",
    "clf.fit(train_data, target)\n",
    "\n",
    "prediction = clf.predict(test)\n",
    "\n",
    "accuracy = accuracy_score(prediction, submission_a)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance 추출\n",
    "\n",
    "print(\"Feature importances:\\n{0}\".format(np.round(clf.feature_importances_,3)))\n",
    "\n",
    "# feature 별 importance 매핑\n",
    "\n",
    "for name, value in zip(train_data.columns\n",
    "                       ,clf.feature_importances_):\n",
    "    print('{0} : {1:.3f}'.format(name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {'max_depth': hp.quniform('max_depth', 1,30,1), \n",
    "                'random_state': hp.quniform('random_state', 1,1000,1),\n",
    "                'n_estimators': hp.quniform('n_estimators', 1,1000,1),\n",
    "                'reg_lambda':hp.quniform('reg_lambda',1,30,1),\n",
    "                'learning_rate':hp.quniform('learning_rate',0.001,0.1,0.001)\n",
    "                #'scale_pos_weight':hp.quniform('scale_pos_weight',1,30,1)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimators=int(search_space['n_estimators'])\n",
    "                            , max_depth=int(search_space['max_depth'])\n",
    "                            ,random_state=int(search_space['random_state'])\n",
    "                            ,reg_lambda=int(search_space['reg_lambda'])\n",
    "                            ,learning_rate=float(search_space['learning_rate'])\n",
    "                            #,scale_pos_weight=int(search_space['scale_pos_weight'])\n",
    "                           )\n",
    "    roc_auc_list= []\n",
    "    xgb_clf.fit(train_data , target)\n",
    "    pred = xgb_clf.predict(test)\n",
    "    submission_a = pd.read_csv('../data/submission_a.csv')\n",
    "    del submission_a['PassengerId']\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(pred, submission_a)\n",
    "    roc_auc_list.append(accuracy)\n",
    "    return -1 * np.mean(roc_auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# fmin()함수를 호출. max_evals지정된 횟수만큼 반복 후 목적함수의 최소값을 가지는 최적 입력값 추출.\n",
    "best = fmin(fn=objective_func,\n",
    "            space=search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=1000, # 최대 반복 횟수를 지정합니다.\n",
    "            trials=trials, \n",
    "            rstate=np.random.default_rng(seed=30))\n",
    "\n",
    "print('best:', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "\n",
    "    'PassengerId':test['PassengerId']\n",
    "    ,'Survived':Y_pred\n",
    "    \n",
    "})\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
